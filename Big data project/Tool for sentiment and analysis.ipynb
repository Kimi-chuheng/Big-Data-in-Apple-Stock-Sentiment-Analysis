{"cells": [{"cell_type": "code", "execution_count": 5, "id": "fdd30c58-ae94-4465-85d3-921262e1a5de", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+----------------+--------------------+\n|               Title|            Time|             Content|\n+--------------------+----------------+--------------------+\n|Messi\u2019s MLS Cup P...|October 23, 2024|['Lionel Messi ma...|\n|The new iPad\u00a0mini...|October 23, 2024|['Beginning today...|\n|Apple celebrates ...|October 17, 2024|['When we started...|\n|Apple expands too...|October 16, 2024|['For the first t...|\n|Apple introduces ...|October 15, 2024|['CUPERTINO, CALI...|\n+--------------------+----------------+--------------------+\nonly showing top 5 rows\n\nProcessing rows 1 to 10...\nProcessing rows 11 to 20...\nProcessing rows 21 to 30...\nProcessing rows 31 to 40...\nProcessing rows 41 to 50...\nProcessing rows 51 to 60...\nProcessing rows 61 to 70...\nProcessing rows 71 to 80...\nProcessing rows 81 to 90...\nProcessing rows 91 to 100...\nProcessing rows 101 to 110...\nProcessing rows 111 to 120...\nProcessing rows 121 to 130...\nProcessing rows 131 to 140...\nProcessing rows 141 to 150...\nProcessing rows 151 to 160...\nProcessing rows 161 to 170...\nProcessing rows 171 to 180...\nProcessing rows 181 to 190...\nProcessing rows 191 to 200...\nProcessing rows 201 to 210...\nProcessing rows 211 to 220...\nProcessing rows 221 to 230...\nProcessing rows 231 to 240...\nProcessing rows 241 to 250...\nProcessing rows 251 to 260...\nProcessing rows 261 to 270...\nProcessing rows 271 to 280...\nProcessing rows 281 to 290...\nProcessing rows 291 to 300...\nProcessing rows 301 to 310...\nProcessing rows 311 to 320...\nProcessing rows 321 to 330...\nProcessing rows 331 to 340...\nProcessing rows 341 to 350...\nProcessing rows 351 to 360...\nProcessing rows 361 to 370...\nProcessing rows 371 to 380...\nProcessing rows 381 to 390...\nProcessing rows 391 to 400...\nProcessing rows 401 to 410...\nProcessing rows 411 to 420...\nProcessing rows 421 to 430...\nProcessing rows 431 to 440...\nProcessing rows 441 to 450...\nProcessing rows 451 to 460...\nProcessing rows 461 to 470...\nProcessing rows 471 to 480...\nProcessing rows 481 to 490...\nProcessing rows 491 to 500...\nProcessing rows 501 to 510...\nProcessing rows 511 to 520...\nProcessing rows 521 to 530...\nProcessing rows 531 to 540...\nProcessing rows 541 to 550...\nProcessing rows 551 to 560...\nProcessing rows 561 to 570...\nProcessing rows 571 to 580...\nProcessing rows 581 to 590...\nProcessing rows 591 to 600...\nProcessing rows 601 to 610...\nProcessing rows 611 to 620...\nProcessing rows 621 to 630...\nProcessing rows 631 to 640...\nProcessing rows 641 to 650...\nProcessing rows 651 to 660...\nProcessing rows 661 to 670...\nProcessing rows 671 to 680...\nProcessing rows 681 to 690...\nProcessing rows 691 to 700...\nProcessing rows 701 to 710...\nProcessing rows 711 to 720...\nProcessing rows 721 to 730...\nProcessing rows 731 to 740...\nProcessing rows 741 to 750...\nProcessing rows 751 to 760...\nProcessing rows 761 to 770...\nProcessing rows 771 to 780...\nProcessing rows 781 to 790...\nProcessing rows 791 to 800...\nProcessing rows 801 to 810...\nProcessing rows 811 to 820...\nProcessing rows 821 to 830...\nProcessing rows 831 to 840...\nProcessing rows 841 to 850...\nProcessing rows 851 to 860...\nProcessing rows 861 to 870...\nProcessing rows 871 to 880...\nProcessing rows 881 to 890...\nProcessing rows 891 to 900...\nProcessing rows 901 to 910...\nProcessing rows 911 to 920...\nProcessing rows 921 to 930...\nProcessing rows 931 to 940...\nProcessing rows 941 to 950...\nProcessing rows 951 to 960...\nProcessing rows 961 to 970...\nProcessing rows 971 to 980...\nProcessing rows 981 to 990...\nProcessing rows 991 to 1000...\nProcessing rows 1001 to 1010...\nProcessing rows 1011 to 1020...\nProcessing rows 1021 to 1030...\nProcessing rows 1031 to 1040...\nProcessing rows 1041 to 1050...\nProcessing rows 1051 to 1060...\nProcessing rows 1061 to 1070...\nProcessing rows 1071 to 1080...\nProcessing rows 1081 to 1090...\nProcessing rows 1091 to 1100...\nProcessing rows 1101 to 1110...\nProcessing rows 1111 to 1120...\nProcessing rows 1121 to 1130...\nProcessing rows 1131 to 1140...\nProcessing rows 1141 to 1150...\nProcessing rows 1151 to 1160...\nProcessing rows 1161 to 1170...\nProcessing rows 1171 to 1180...\nProcessing rows 1181 to 1190...\nProcessing rows 1191 to 1200...\nProcessing rows 1201 to 1210...\nProcessing rows 1211 to 1220...\nProcessing rows 1221 to 1230...\nProcessing rows 1231 to 1240...\nProcessing rows 1241 to 1250...\nProcessing rows 1251 to 1260...\nProcessing rows 1261 to 1270...\nProcessing rows 1271 to 1280...\nProcessing rows 1281 to 1290...\nProcessing rows 1291 to 1300...\nProcessing rows 1301 to 1310...\nProcessing rows 1311 to 1320...\nProcessing rows 1321 to 1330...\nProcessing rows 1331 to 1340...\nProcessing rows 1341 to 1350...\nProcessing rows 1351 to 1360...\nProcessing rows 1361 to 1370...\nProcessing rows 1371 to 1380...\nProcessing rows 1381 to 1390...\nProcessing rows 1391 to 1400...\nProcessing rows 1401 to 1410...\nProcessing rows 1411 to 1420...\nProcessing rows 1421 to 1430...\nProcessing rows 1431 to 1440...\nProcessing rows 1441 to 1450...\nProcessing rows 1451 to 1460...\nProcessing rows 1461 to 1470...\nProcessing rows 1471 to 1480...\nProcessing rows 1481 to 1490...\nProcessing rows 1491 to 1500...\nProcessing rows 1501 to 1510...\nProcessing rows 1511 to 1520...\nProcessing rows 1521 to 1530...\nProcessing rows 1531 to 1540...\nProcessing rows 1541 to 1550...\nProcessing rows 1551 to 1560...\nProcessing rows 1561 to 1570...\nProcessing rows 1571 to 1580...\nProcessing rows 1581 to 1590...\nProcessing rows 1591 to 1600...\nProcessing rows 1601 to 1610...\nProcessing rows 1611 to 1620...\nProcessing rows 1621 to 1630...\nProcessing rows 1631 to 1640...\nProcessing rows 1641 to 1650...\nProcessing rows 1651 to 1660...\nProcessing rows 1661 to 1670...\nProcessing rows 1671 to 1680...\nProcessing rows 1681 to 1690...\nProcessing rows 1691 to 1700...\nProcessing rows 1701 to 1710...\nProcessing rows 1711 to 1720...\nProcessing rows 1721 to 1730...\nProcessing rows 1731 to 1740...\nProcessing rows 1741 to 1750...\nProcessing rows 1751 to 1760...\nProcessing rows 1761 to 1770...\nProcessing rows 1771 to 1780...\nProcessing rows 1781 to 1790...\nProcessing rows 1791 to 1800...\nProcessing rows 1801 to 1810...\nProcessing rows 1811 to 1820...\nProcessing rows 1821 to 1830...\nProcessing rows 1831 to 1840...\nProcessing rows 1841 to 1850...\nProcessing rows 1851 to 1860...\nProcessing rows 1861 to 1870...\nProcessing rows 1871 to 1880...\nProcessing rows 1881 to 1890...\nProcessing rows 1891 to 1900...\nProcessing rows 1901 to 1910...\nProcessing rows 1911 to 1920...\nProcessing rows 1921 to 1930...\nProcessing rows 1931 to 1940...\nProcessing rows 1941 to 1950...\nProcessing rows 1951 to 1960...\nProcessing rows 1961 to 1970...\nProcessing rows 1971 to 1980...\nProcessing rows 1981 to 1990...\nProcessing rows 1991 to 2000...\nProcessing rows 2001 to 2010...\nProcessing rows 2011 to 2020...\nProcessing rows 2021 to 2030...\nProcessing rows 2031 to 2040...\nProcessing rows 2041 to 2050...\nProcessing rows 2051 to 2060...\nProcessing rows 2061 to 2070...\nProcessing rows 2071 to 2080...\nProcessing rows 2081 to 2090...\nProcessing rows 2091 to 2100...\nProcessing rows 2101 to 2110...\nProcessing rows 2111 to 2120...\nProcessing rows 2121 to 2130...\nProcessing rows 2131 to 2140...\nProcessing rows 2141 to 2150...\nProcessing rows 2151 to 2160...\nProcessing rows 2161 to 2170...\nProcessing rows 2171 to 2180...\nProcessing rows 2181 to 2190...\nProcessing rows 2191 to 2200...\nProcessing rows 2201 to 2210...\nProcessing rows 2211 to 2220...\nProcessing rows 2221 to 2230...\nProcessing rows 2231 to 2240...\nProcessing rows 2241 to 2250...\nProcessing rows 2251 to 2260...\nProcessing rows 2261 to 2270...\nProcessing rows 2271 to 2280...\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Sentiment score result\uff1a\n               Time  sentiment\n0  October 23, 2024        0.4\n1  October 23, 2024        0.3\n2  October 17, 2024        0.4\n3  October 16, 2024        0.2\n4  October 15, 2024        0.5\n"}, {"name": "stderr", "output_type": "stream", "text": "24/10/30 01:37:50 WARN ApplicationMaster: Reporter thread fails 1 time(s) in a row.\njava.io.InterruptedIOException: Call interrupted\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1560) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1512) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1409) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139) ~[hadoop-client-api-3.3.6.jar:?]\n\tat com.sun.proxy.$Proxy40.allocate(Unknown Source) ~[?:?]\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:78) ~[hadoop-client-api-3.3.6.jar:?]\n\tat jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source) ~[?:?]\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-client-api-3.3.6.jar:?]\n\tat com.sun.proxy.$Proxy41.allocate(Unknown Source) ~[?:?]\n\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-client-api-3.3.6.jar:?]\n\tat org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:426) ~[spark-yarn_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.deploy.yarn.ApplicationMaster.org$apache$spark$deploy$yarn$ApplicationMaster$$allocationThreadImpl(ApplicationMaster.scala:578) [spark-yarn_2.12-3.5.1.jar:3.5.1]\n\tat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:648) [spark-yarn_2.12-3.5.1.jar:3.5.1]\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom google.cloud import language_v1\nimport pandas as pd\n\n# Create SparkSession\nspark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n\n# Read data from GCS\ndf = spark.read.csv(\"gs://datakimmy1/apple_news.csv\", header=True, inferSchema=True)\n\n# Show the first few rows of data\ndf.show(5)\n\n# Define the sentiment analysis function\ndef analyze_sentiment(text):\n    client = language_v1.LanguageServiceClient()\n    document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n    response = client.analyze_sentiment(request={'document': document})\n    return response.document_sentiment.score   # Return sentiment score\n\n# Use UDF to apply sentiment analysis to the Content column\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import FloatType\n\n# Create UDF\nsentiment_udf = udf(analyze_sentiment, FloatType())\n\n# Add a new column to store sentiment scores\ndf_with_sentiment = df.withColumn(\"sentiment\", sentiment_udf(col(\"Content\")))\n\n# Progress tracking\nnum_rows = df.count()  # Get total number of rows\nfor i in range(0, num_rows, 10):\n    print(f\"Processing rows {i + 1} to {min(i + 10, num_rows)}...\")\n\n# Retain only the \"Time\" and \"sentiment\" columns\ndf_final = df_with_sentiment.select(\"Time\", \"sentiment\")\n\n# Convert the DataFrame with sentiment scores to a Pandas DataFrame for local saving\npandas_df = df_final.toPandas()\n\n# Print the first few rows of the final result\nprint(\"Sentiment score result:\")\nprint(pandas_df.head())\n\n# Save to GCS\noutput_path = \"gs://datakimmy1/sentiment_analysis_results.csv\"\ndf_final.write.csv(output_path, header=True, mode='overwrite')\n\n# Close Spark Session\nspark.stop()\n\n"}, {"cell_type": "code", "execution_count": null, "id": "84de827e-e297-4334-80ae-12af7e279b9a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/10/30 05:16:11 ERROR ClusterManager: Could not initialize cluster nodes=[cluster-adf4-w-0.us-central1-f.c.voltaic-athlete-440119-h1.internal, cluster-adf4-w-1.us-central1-f.c.voltaic-athlete-440119-h1.internal] nodeHostName=cluster-adf4-m.us-central1-f.c.voltaic-athlete-440119-h1.internal nodeHostAddress=10.128.0.34 currentNodeIndex=null\n24/10/30 05:16:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n24/10/30 05:16:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"}], "source": "from pyspark.sql import SparkSession \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create SparkSession\nspark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n\n# Read the sentiment score table from GCS\nsentiment_df = spark.read.csv(\"gs://datakimmy1/sentiment_analysis_results.csv\", header=True, inferSchema=True)\n\n# Convert the DataFrame to Pandas format and ensure the date column format is consistent\nsentiment_pd_df = sentiment_df.select(\"Time\", \"sentiment\").toPandas()\n\n# Convert the sentiment column to float type\nsentiment_pd_df['sentiment'] = sentiment_pd_df['sentiment'].astype(float)\n\n# Generate a histogram of sentiment scores\nplt.figure(figsize=(12, 6))\nplt.hist(sentiment_pd_df['sentiment'].dropna(), bins=30, alpha=0.7, color='blue', edgecolor='black')\nplt.title(\"Distribution of Apple News' Sentiment Scores\")\nplt.xlabel(\"Sentiment Score\")\nplt.ylabel(\"Frequency\")\nplt.grid(axis='y', alpha=0.75)\nplt.show()\n"}, {"cell_type": "code", "execution_count": null, "id": "85fdb203-4af5-49af-b477-d8056eeaeb90", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession \nfrom pyspark.sql.functions import col\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create SparkSession\nspark = SparkSession.builder.appName(\"SentimentStockAnalysis\").getOrCreate()\n\n# Read the sentiment score table and Apple stock price table from GCS\nsentiment_df = spark.read.csv(\"gs://datakimmy1/sentiment_analysis_results.csv\", header=True, inferSchema=True)\nstock_df = spark.read.csv(\"gs://datakimmy1/AppleStockPrice.csv\", header=True, inferSchema=True)\n\n# Rename the date column in the sentiment score table for merging\nsentiment_df = sentiment_df.withColumnRenamed(\"Time\", \"Date\")\n\n# Convert DataFrames to Pandas format and ensure the date columns are consistent\nsentiment_pd_df = sentiment_df.select(\"Date\", \"sentiment\").toPandas()\nstock_pd_df = stock_df.select(\"Date\", \"Close\").toPandas()\n\n# Convert the Date columns to datetime type\nsentiment_pd_df['Date'] = pd.to_datetime(sentiment_pd_df['Date'], dayfirst=True, errors='coerce')\nstock_pd_df['Date'] = pd.to_datetime(stock_pd_df['Date'], dayfirst=True, errors='coerce')\n\n# Generate a complete date range\nfull_date_range = pd.DataFrame(pd.date_range(start=sentiment_pd_df[\"Date\"].min(), end=stock_pd_df[\"Date\"].max()), columns=[\"Date\"])\n\n# Merge data\nmerged_df = full_date_range.merge(sentiment_pd_df, on=\"Date\", how=\"left\").merge(stock_pd_df, on=\"Date\", how=\"left\")\nprint(merged_df.tail(5))\n\n# Define a function to calculate the average future gain over a specified window\ndef calculate_avg_future_gain(df, window=3):\n    future_gains = []\n    for i, row in df.iterrows():\n        current_price = row['Close']\n        future_prices = df['Close'].iloc[i + 1:i + 1 + window]  # Future closing prices for the next few days\n        if pd.notna(row['sentiment']) and len(future_prices) == window:\n            # Only calculate if sentiment is not NaN\n            gains = (future_prices.values - current_price) / current_price * 100  # Percentage change\n            avg_gain = np.mean(gains)\n            future_gains.append(avg_gain)\n        else:\n            # If sentiment score is NaN or there aren't enough future days, skip\n            future_gains.append(np.nan)\n    df['avg_future_gain'] = future_gains\n    return df\n\n# Calculate the average future gain for each date with a sentiment value\nmerged_df = calculate_avg_future_gain(merged_df)\n\n# Calculate average future gains based on different sentiment scores\navg_gain_by_sentiment = merged_df.groupby('sentiment')['avg_future_gain'].mean().reset_index()\n\n# Handle NaN values by filling with linear interpolation\navg_gain_by_sentiment['avg_future_gain'] = avg_gain_by_sentiment['avg_future_gain'].interpolate(method='linear')\n\n# Plot a line graph\nplt.figure(figsize=(12, 6))\nplt.plot(avg_gain_by_sentiment['sentiment'], avg_gain_by_sentiment['avg_future_gain'], marker='o', linestyle='-', color='b')\nplt.xlabel(\"Sentiment Score\")\nplt.ylabel(\"Average Future Gain (%)\")\nplt.title(\"Average Future Stock Price Change by Sentiment Score\")\nplt.grid(True)\nplt.xticks(avg_gain_by_sentiment['sentiment'])  # Set x-axis ticks to sentiment scores\nplt.show()\n\n# Stop the Spark Session\nspark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "0eb5f0bc-748c-4987-8161-4f10f1bed283", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}